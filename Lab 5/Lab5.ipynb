{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Import the necessary libraries**"
      ],
      "metadata": {
        "id": "stKL0jP9OzSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import math\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from itertools import combinations\n",
        "wnl = nltk.stem.WordNetLemmatizer()\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqoU-7ank_CR",
        "outputId": "255f6c8a-6222-4f1b-fae7-68a2f78fcc7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Declare the pairs and pos mapping to Wordnet**\n",
        "\n"
      ],
      "metadata": {
        "id": "HsftMniaS2Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of (lemma, category) pairs\n",
        "pairs = [\n",
        "    ('the', 'DT'), ('man', 'NN'), ('swim', 'VB'), ('with', 'PR'), ('a', 'DT'),\n",
        "    ('girl', 'NN'), ('and', 'CC'), ('a', 'DT'), ('boy', 'NN'), ('whilst', 'PR'),\n",
        "    ('the', 'DT'), ('woman', 'NN'), ('walk', 'VB')\n",
        "]\n",
        "\n",
        "# Mapping from custom POS tags to WordNet POS tags\n",
        "pos_mapping = {\n",
        "    'NN': wn.NOUN,    # Nouns\n",
        "    'VB': wn.VERB,    # Base form of verbs\n",
        "    'JJ': wn.ADJ,     # Adjectives\n",
        "    'RB': wn.ADV      # Adverbs\n",
        "}"
      ],
      "metadata": {
        "id": "TV_dYUiNS9sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filtering out closed-class words**\n"
      ],
      "metadata": {
        "id": "d5SKPsapTMx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_closed_words(list_of_lemmas, allowed_lemmas):\n",
        "    # Filter out lemmas where the category (POS tag) is not in the allowed_lemmas keys\n",
        "    filtered_lemmas = [(lemma, category) for lemma, category in list_of_lemmas if category in allowed_lemmas.keys()]\n",
        "    return filtered_lemmas\n",
        "\n",
        "# Filter the pairs\n",
        "filtered_pairs = remove_closed_words(pairs, pos_mapping)\n",
        "print(\"Filtered Pairs:\")\n",
        "print(filtered_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV7-AXoTTuZ1",
        "outputId": "ea1561d5-a161-405a-975a-7001fe8c9123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Pairs:\n",
            "[('man', 'NN'), ('swim', 'VB'), ('girl', 'NN'), ('boy', 'NN'), ('woman', 'NN'), ('walk', 'VB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get most frequent synset**"
      ],
      "metadata": {
        "id": "LviY5f41UHX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_frequent_synset(word, pos):\n",
        "    synsets = wn.synsets(word, pos=pos)\n",
        "    if synsets:\n",
        "        # The first synset is the most frequent one\n",
        "        return synsets[0]\n",
        "    else:\n",
        "        return None  # No synset found for this word with the given POS\n"
      ],
      "metadata": {
        "id": "U-makQGIUbOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Find the absolute max for each differenc POS in Wordent**"
      ],
      "metadata": {
        "id": "cTeY1zsMUggq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_max_depth(pos):\n",
        "    max_depth = 0\n",
        "    for synset in wn.all_synsets(pos):\n",
        "        # Get the length of the longest hypernym path for this synset\n",
        "        for path in synset.hypernym_paths():\n",
        "            max_depth = max(max_depth, len(path))\n",
        "    return max_depth\n",
        "\n",
        "# Calculate the maximum depth for relevant POS\n",
        "max_depth_verbs = find_max_depth(wn.VERB)\n",
        "max_depth_nouns = find_max_depth(wn.NOUN)\n",
        "max_depth_adjs = find_max_depth(wn.ADJ)\n",
        "max_depth_advs = find_max_depth(wn.ADV)\n",
        "\n",
        "# Map POS to their max depth and calculate max_lch\n",
        "max_depth_mapping = {\n",
        "    wn.NOUN: max_depth_nouns,\n",
        "    wn.VERB: max_depth_verbs,\n",
        "    wn.ADJ: max_depth_adjs,\n",
        "    wn.ADV: max_depth_advs\n",
        "}"
      ],
      "metadata": {
        "id": "WsR4llFQUw57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compute the similarities**"
      ],
      "metadata": {
        "id": "otMrHJIIXB8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute similarities between two synsets\n",
        "def compute_similarities(syn1, syn2):\n",
        "    # Compute the Least Common Subsumer\n",
        "    lcs = syn1.lowest_common_hypernyms(syn2)\n",
        "    similarities = {}\n",
        "\n",
        "    # Path Similarity\n",
        "    path_sim = syn1.path_similarity(syn2)\n",
        "    similarities['path_similarity'] = path_sim\n",
        "\n",
        "    # Leacock-Chodorow Similarity\n",
        "    if syn1.pos() == syn2.pos():\n",
        "        max_depth = max_depth_mapping[syn1.pos()]\n",
        "        max_lch = math.log(2 * max_depth)\n",
        "        lch_sim = syn1.lch_similarity(syn2)\n",
        "        normalized_lch = (lch_sim - 0) / (max_lch - 0)  # Normalize between min=0 and max_lch\n",
        "    else:\n",
        "        normalized_lch = None\n",
        "    similarities['lch_similarity'] = normalized_lch\n",
        "\n",
        "    # Wu-Palmer Similarity\n",
        "    wup_sim = syn1.wup_similarity(syn2)\n",
        "    similarities['wup_similarity'] = wup_sim\n",
        "\n",
        "    # Lin Similarity (might throw an error if information content is missing)\n",
        "    try:\n",
        "        lin_sim = syn1.lin_similarity(syn2, brown_ic)\n",
        "    except:\n",
        "        lin_sim = None\n",
        "    similarities['lin_similarity'] = lin_sim\n",
        "\n",
        "    return similarities, lcs"
      ],
      "metadata": {
        "id": "Ln4Z0NzNZK4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyze similarities using the most frequent synsets**"
      ],
      "metadata": {
        "id": "BSflRuYrZ5Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyze similarities using the most frequent synsets\n",
        "def analyze_similarities_mfs(pairs, pos_mapping):\n",
        "    results = []\n",
        "    # Generate all unique combinations of word pairs using built-in function combinations\n",
        "    for (word1, pos1), (word2, pos2) in combinations(pairs, 2):\n",
        "        wn_pos1 = pos_mapping.get(pos1)\n",
        "        wn_pos2 = pos_mapping.get(pos2)\n",
        "\n",
        "        syn1 = get_most_frequent_synset(word1, wn_pos1)\n",
        "        syn2 = get_most_frequent_synset(word2, wn_pos2)\n",
        "\n",
        "\n",
        "        similarities, lcs = compute_similarities(syn1, syn2)\n",
        "\n",
        "        # If LCS is found, continue with the analysis, otherwise add a custom message\n",
        "        if not lcs:\n",
        "            results.append({\n",
        "                'word1': word1,\n",
        "                'synset1': syn1,\n",
        "                'word2': word2,\n",
        "                'synset2': syn2,\n",
        "                'lcs': None,\n",
        "                'similarities': None,\n",
        "                'message': f\"Cannot compute similarities because the synsets for '{word1}' and '{word2}' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\"\n",
        "            })\n",
        "        else:\n",
        "            results.append({\n",
        "                'word1': word1,\n",
        "                'synset1': syn1,\n",
        "                'word2': word2,\n",
        "                'synset2': syn2,\n",
        "                'lcs': lcs,\n",
        "                'similarities': similarities,\n",
        "                'message': None\n",
        "            })\n",
        "    return results"
      ],
      "metadata": {
        "id": "qFzlHIj3Z5YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Show the results**"
      ],
      "metadata": {
        "id": "9oe2vE0KawDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(results):\n",
        "    for res in results:\n",
        "        word1 = res['word1']\n",
        "        syn1 = res['synset1']\n",
        "        word2 = res['word2']\n",
        "        syn2 = res['synset2']\n",
        "        lcs = res['lcs']\n",
        "        sims = res['similarities']\n",
        "        message = res['message']\n",
        "\n",
        "        print(f\"\\nWord Pair: '{word1}' ({syn1.name()}) - '{word2}' ({syn2.name()})\")\n",
        "\n",
        "        if lcs:\n",
        "            print(f\"Least Common Subsumer (LCS): {lcs[0].name()}\")\n",
        "            print(\"Similarity Measures:\")\n",
        "            print(f\"  Path Similarity: {sims['path_similarity']}\")\n",
        "            print(f\"  Leacock-Chodorow Similarity (Normalized): {sims['lch_similarity']}\")\n",
        "            print(f\"  Wu-Palmer Similarity: {sims['wup_similarity']}\")\n",
        "            print(f\"  Lin Similarity: {sims['lin_similarity']}\")\n",
        "            print(\"-------------------------------------------------\")\n",
        "        else:\n",
        "            # If no common LCS, print the custom message\n",
        "            print(\"Least Common Subsumer (LCS): None\")\n",
        "            print(message)\n",
        "            print(\"-------------------------------------------------\")\n",
        "\n",
        "# Run the analysis using the most frequent synsets\n",
        "similarity_results_mfs = analyze_similarities_mfs(filtered_pairs, pos_mapping)\n",
        "display_results(similarity_results_mfs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bao_-llVK6pg",
        "outputId": "6fdb55e6-c2f5-4bb4-dbb7-419efebac232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Pair: 'man' (man.n.01) - 'swim' (swim.v.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'man' and 'swim' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'man' (man.n.01) - 'girl' (girl.n.01)\n",
            "Least Common Subsumer (LCS): adult.n.01\n",
            "Similarity Measures:\n",
            "  Path Similarity: 0.25\n",
            "  Leacock-Chodorow Similarity (Normalized): 0.6102915062989643\n",
            "  Wu-Palmer Similarity: 0.631578947368421\n",
            "  Lin Similarity: 0.7135111237276783\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'man' (man.n.01) - 'boy' (male_child.n.01)\n",
            "Least Common Subsumer (LCS): male.n.02\n",
            "Similarity Measures:\n",
            "  Path Similarity: 0.3333333333333333\n",
            "  Leacock-Chodorow Similarity (Normalized): 0.6882778097361639\n",
            "  Wu-Palmer Similarity: 0.6666666666666666\n",
            "  Lin Similarity: 0.7294717876200584\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'man' (man.n.01) - 'woman' (woman.n.01)\n",
            "Least Common Subsumer (LCS): adult.n.01\n",
            "Similarity Measures:\n",
            "  Path Similarity: 0.3333333333333333\n",
            "  Leacock-Chodorow Similarity (Normalized): 0.6882778097361639\n",
            "  Wu-Palmer Similarity: 0.6666666666666666\n",
            "  Lin Similarity: 0.7870841372982784\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'man' (man.n.01) - 'walk' (walk.v.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'man' and 'walk' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'swim' (swim.v.01) - 'girl' (girl.n.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'swim' and 'girl' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'swim' (swim.v.01) - 'boy' (male_child.n.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'swim' and 'boy' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'swim' (swim.v.01) - 'woman' (woman.n.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'swim' and 'woman' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'swim' (swim.v.01) - 'walk' (walk.v.01)\n",
            "Least Common Subsumer (LCS): travel.v.01\n",
            "Similarity Measures:\n",
            "  Path Similarity: 0.3333333333333333\n",
            "  Leacock-Chodorow Similarity (Normalized): 0.6628054829415044\n",
            "  Wu-Palmer Similarity: 0.3333333333333333\n",
            "  Lin Similarity: 0.4910052007916556\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'girl' (girl.n.01) - 'boy' (male_child.n.01)\n",
            "Least Common Subsumer (LCS): person.n.01\n",
            "Similarity Measures:\n",
            "  Path Similarity: 0.16666666666666666\n",
            "  Leacock-Chodorow Similarity (Normalized): 0.5003759850270564\n",
            "  Wu-Palmer Similarity: 0.631578947368421\n",
            "  Lin Similarity: 0.2927280671561499\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'girl' (girl.n.01) - 'woman' (woman.n.01)\n",
            "Least Common Subsumer (LCS): woman.n.01\n",
            "Similarity Measures:\n",
            "  Path Similarity: 0.5\n",
            "  Leacock-Chodorow Similarity (Normalized): 0.7981933310080719\n",
            "  Wu-Palmer Similarity: 0.631578947368421\n",
            "  Lin Similarity: 0.9067798595489287\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'girl' (girl.n.01) - 'walk' (walk.v.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'girl' and 'walk' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'boy' (male_child.n.01) - 'woman' (woman.n.01)\n",
            "Least Common Subsumer (LCS): person.n.01\n",
            "Similarity Measures:\n",
            "  Path Similarity: 0.2\n",
            "  Leacock-Chodorow Similarity (Normalized): 0.5498006298445022\n",
            "  Wu-Palmer Similarity: 0.6666666666666666\n",
            "  Lin Similarity: 0.31842335630818425\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'boy' (male_child.n.01) - 'walk' (walk.v.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'boy' and 'walk' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n",
            "\n",
            "Word Pair: 'woman' (woman.n.01) - 'walk' (walk.v.01)\n",
            "Least Common Subsumer (LCS): None\n",
            "Cannot compute similarities because the synsets for 'woman' and 'walk' do not have the same POS_tag and a least common subsumer (LCS) in WordNet.\n",
            "-------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysing and Conclusions**\n",
        "\n",
        "* In our given pairs we have different lemmas together with their POS_tag , but\n",
        "since Wordnet uses only open-classes(nouns, verbs, adjectives and adverbs) we don't have to consider all of them. So we filter the pairs in a new list containing only the lemmas and POS_tags allowed by Wordnet.\n",
        "\n",
        "\n",
        "* We need to find the most frequent synset for each lemma because words in natural language often have multiple meanings, and each distinct meaning is represented by a different synset in WordNet. In semantic similarity calculations (such as computing the Least Common Subsumer or other similarity metrics), the most frequent synset is generally assumed to represent the most common or likely meaning of the word in everyday usage. From the documentation of WordNet it is stated that the synsets are sorted based on their frequency, so we used the first one (index 0).\n",
        "\n",
        "* Before computing similarities, we know that Leacock-Chodorow Similarity needs normalization. So to normalize it we have used a Min-Max approach that in our case would involve finding the minimum and maximum possible values of the LCH similarity, then scaling the actual LCH similarity to the [0, 1] range.\n",
        " 1. The minimum possible LCH similarity occurs when the two synsets are maximally distant in the hierarchy so we consider the min_lch = 0\n",
        "         * Since the shortest common path will be 2 * MaxDepth, and -log(2 * MaxDepth/ 2 * MaxDepth) = 0\n",
        " 2.  The maximum LCH similarity value occurs when two synsets are the same. In that case, the shortest path is 1 , so the formula will now be:\n",
        "\n",
        "            * LCH(s,s) = - log(1/2 * MaxDepth) = log (2 * MaxDepth)\n",
        " 3. Normalize the Leacock-Chodorow Similarity by using the new formula instead:\n",
        "            * NomalizedLCH(s1, s2) = (LCH(s1,s2) - min_lch)/(max_lch - min_lch)\n",
        "            where as we said : min_lch is 0 and max_lch is log(2*MaxDepth)\n",
        "\n",
        "  4. In order to find the absolute max depth we created the above function that for each POS_tag finds the max depth, beacuse max depth is different for different POS_tags. For example in our cases , max_lch for nouns was 20 and for verbs was 13, so we have to rely on these different values for different POS_tag. Since we are using an approximation for nouns, if you compute the similiarity of two similiar words will not be exactly 1.0 but it is closed to 1 (0.99).\n",
        "\n",
        "\n",
        "* The other step is to compute the Least Common Subsumer (LCS) and all the similiarities. Let's explain shortly what each of them do and try to find the better one for our case:\n",
        "\n",
        "    1. ***LCS*** in WordNet is the most specific ancestor synset that two synsets share in the hypernym hierarchy. So we can say it is the most specific concept that is a hypernym of both synsets.\n",
        "    2. ***Path Similarity*** measures the similarity between two synsets based on the shortest path connecting them in the WordNet hierarchy. The shorter the path, the more similar the synsets are assumed to be. It has a limitations that it treats all edges in the hierarchy equally so a long path through very specific synsets may get the same score as a short path through more general synsets.\n",
        "    3. ***Leacock-Chodorow Similarity*** measures the similarity between two synsets based on the shortest path between them and the maximum depth of the taxonomy. It takes into account the overall size of the taxonomy and normalizes the path length based on it. It is better than path similarity at handling hierarchies of different sizes and complexity.\n",
        "    4. ***Wu-Palmer*** calculates the similarity between two synsets based on the depth of LCS and the depths of the individual synsets, so two synsets are more similar if they share a common ancestor that is closer to them in the hierarchy.It focuses on the structural similarity of two synsets based on how far they are from their common ancestor, making it effective in capturing hierarchical relations.\n",
        "    5. ***Lin similarity*** measures the similarity between two synsets based on their information content. The IC of a synset is derived from the probability of encountering that synset in a large corpus of text, where more specific synsets have higher information content. Lin similarity compares how much information is shared by the two synsets relative to their total information content. It takes into consideration the hierarchical structure of WordNet and statistical information from large corpora, making it sensitive to how commonly or rarely concepts are used in the language.\n",
        "            *For the lemmas that does not have the same POS_tag, they also does not have a least common subsumer (LCS)\n",
        "            in WordNet, so we cannot compute their similarities.\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "We think that Path similarity tends to give lower scores for very close synsets (for example for girl and women it gives only 0.5 which is low compared to other measures). In contrast, Leacock-Chodorow similarity can be considered a better option comapred to the Path similarity, but since we also need to normalize it, it can take longer time to compute.\n",
        "\n",
        "We think that Wu-Palmer and Lin similarities tend to perform better based on the lemma pairs. Since the Lin similarity is based on the information content (IC), it may be better to use it when we are looking for contex-aware measure how words are used in real world texts. On the other hand, Wu-Palmer consideres hierarchical structure and the closeness in taxonomy, hence it may be a good option when working with WordNet's structure.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mO-TZwX1TgT8"
      }
    }
  ]
}